# Model_Analysis
This repo will have a set of tools across multiple AI models, platforms, and settings for performing in depth model evaluation.

## Goal of Repo


Thousands of ML models are produced every year. All have different architectures, training paradigms, data modalities, and configurations. 

However, the end result (after training) for every ML model practitioner is a set of weights and the task of deciding how to evaluate the produced model.

Oftentimes, this is not as simple as producing an accuracy score on a given dataset. Depending on the audience, different types of analyses may be required such as layer wise understanding, weight distributions, visualization plots, and etc.

These types of analyses are often written on a repo specific basis and we usually only write enough evaluation for our given use cases. This is fine for repos that we personally use, but when the scope of our problem changes or we start to look at models from different sources, it becomes a headache to have to redefine our evaluation process constantly.

Wouldn't it be better to specify a model and 

The goal of this repo is multifaceted:


## Organization of Repo